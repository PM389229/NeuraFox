import streamlit as st
import whisper
import re
import tempfile, os, torch, wave, sounddevice as sd
import json
import numpy as np
from vosk import Model, KaldiRecognizer
from transformers import AutoProcessor, SeamlessM4TModel, AutoModelForSpeechSeq2Seq,VitsModel, AutoTokenizer
import subprocess
import queue
import time
import soundfile as sf
import requests, zipfile, io

# =======================
# CONFIGURATION ET DONN√âES DES LANGUES
# =======================

LANGUAGES_DATA = {
    "Fran√ßais": {"seamless_code": "__fra__", "vosk_model_id": "vosk-model-small-fr-0.22", "tts_lang_code": "FR"},
    "Anglais": {"seamless_code": "__eng__", "vosk_model_id": "vosk-model-small-en-us-0.22", "tts_lang_code": "EN"},
    "Japonais": {"seamless_code": "__jpn__", "vosk_model_id": None, "tts_lang_code": "JA"},
    "Allemand": {"seamless_code": "__deu__", "vosk_model_id": "vosk-model-small-de-0.21", "tts_lang_code": "DE"},
    "Espagnol": {"seamless_code": "__spa__", "vosk_model_id": "vosk-model-small-es-0.42", "tts_lang_code": "ES"},
    "Russe": {"seamless_code": "__rus__", "vosk_model_id": "vosk-model-small-ru-0.10", "tts_lang_code": "RU"},
    # Nouveaux ajouts
    
    "Portugais": {"seamless_code": "__por__", "vosk_model_id": "vosk-model-small-pt-0.3", "tts_lang_code": "PT"},
    "Mandarin": {"seamless_code": "__cmn__", "vosk_model_id": "vosk-model-small-cn-0.22", "tts_lang_code": "ZH"},
    "Arabe": {"seamless_code": "__arb__", "vosk_model_id": "vosk-model-ar-mgb2-0.4", "tts_lang_code": "AR"},
}

# Mod√®les MMS-TTS (chinois retir√© car on utilise MeloTTS)
MMS_TTS_MODELS = {
    "FR": "facebook/mms-tts-fra",
    "EN": "facebook/mms-tts-eng",
    "DE": "facebook/mms-tts-deu",
    "ES": "facebook/mms-tts-spa",
    "RU": "facebook/mms-tts-rus",
    # Ajouts
    
    "PT": "facebook/mms-tts-por",
    "AR": "facebook/mms-tts-ara",
    # ZH retir√© - on utilise MeloTTS
}

VOSK_URLS = {
    "Fran√ßais": "https://alphacephei.com/vosk/models/vosk-model-small-fr-0.22.zip",
    "Anglais": "https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip",
    "Allemand": "https://alphacephei.com/vosk/models/vosk-model-small-de-0.15.zip",
    "Espagnol": "https://alphacephei.com/vosk/models/vosk-model-small-es-0.42.zip",
    "Russe": "https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip", 
    # Ajouts
    
    "Portugais": "https://alphacephei.com/vosk/models/vosk-model-small-pt-0.3.zip",
    "Mandarin": "https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip",
    "Arabe": "https://alphacephei.com/vosk/models/vosk-model-ar-mgb2-0.4.zip",
}

base_path = os.path.dirname(os.path.abspath(__file__))
MODEL_LOCAL_DIR = os.path.join(base_path, "hf-seamless-m4t-medium")
CACHE_DIR = os.path.join(base_path, "cache")
os.makedirs(CACHE_DIR, exist_ok=True)

SILENCE_THRESHOLD = 0.01
SILENCE_DURATION = 2.0
MAX_RECORDING_TIME = 60.0
SAMPLE_RATE = 16000

# =======================
# FONCTIONS DE CHARGEMENT DE MOD√àLES
# =======================

@st.cache_resource
def get_translator_pipeline():
    print("LOG: Chargement du mod√®le SeamlessM4T...")
    if not os.path.exists(MODEL_LOCAL_DIR):
        st.error(f"Le mod√®le SeamlessM4T est introuvable dans {MODEL_LOCAL_DIR}")
        st.stop()
    processor = AutoProcessor.from_pretrained(MODEL_LOCAL_DIR, local_files_only=True)
    model = SeamlessM4TModel.from_pretrained(MODEL_LOCAL_DIR, local_files_only=True)
    print("LOG: Mod√®le SeamlessM4T charg√© avec succ√®s")
    return model, processor

@st.cache_resource
def load_whisper_model():
    print("LOG: Chargement du mod√®le Whisper...")
    model = whisper.load_model("small", device="cpu")
    print("LOG: Mod√®le Whisper charg√©")
    return model

def download_vosk_model(lang_name):
    vosk_id = LANGUAGES_DATA[lang_name]["vosk_model_id"]
    if vosk_id is None:
        print(f"LOG: Pas de mod√®le Vosk pour {lang_name}")
        return None

    path = os.path.join(CACHE_DIR, vosk_id)
    if os.path.exists(path) and os.listdir(path):
        print(f"LOG: Mod√®le Vosk pour {lang_name} d√©j√† pr√©sent dans {path}")
        try:
            return Model(path)
        except Exception as e:
            print(f"ERROR: √âchec chargement mod√®le existant: {e}")

    url = VOSK_URLS.get(lang_name)
    if url is None:
        print(f"LOG: Pas d'URL pour {lang_name}")
        return None

    st.info(f"T√©l√©chargement du mod√®le Vosk pour {lang_name}...")
    print(f"LOG: T√©l√©chargement Vosk depuis {url}")
    r = requests.get(url)
    with zipfile.ZipFile(io.BytesIO(r.content)) as zf:
        zf.extractall(CACHE_DIR)

        # D√©tecter automatiquement le vrai dossier extrait
        subdirs = [name.split("/")[0] for name in zf.namelist() if "/" in name]
        if subdirs:
            path = os.path.join(CACHE_DIR, subdirs[0])

    print(f"LOG: Mod√®le Vosk {lang_name} extrait dans {path}")

    # V√©rifier que le dossier contient bien des fichiers de mod√®le
    required_files = ["am", "conf", "graph"]
    if not all(os.path.exists(os.path.join(path, f)) for f in required_files):
        print(f"ERROR: Dossier Vosk {path} ne contient pas tous les fichiers n√©cessaires")
        return None

    try:
        model = Model(path)
        print(f"LOG: Mod√®le Vosk {lang_name} charg√© avec succ√®s")
        return model
    except Exception as e:
        print(f"ERROR: Impossible de cr√©er le mod√®le Vosk: {e}")
        return None

def download_mms_model(lang_code):
    """
    T√©l√©charge ou pr√©pare le mod√®le TTS pour la langue donn√©e.
    
    - Pour "JA" (Japonais) et "ZH" (Chinois) ‚Üí utilise MeloTTS via script externe
    - Pour les autres langues ‚Üí MMS-TTS (VITS)
    """
    if lang_code in ["JA", "ZH"]:
        # V√©rifier si le mod√®le est d√©j√† dans le cache
        model_path = os.path.join(CACHE_DIR, f"melo_{lang_code.lower()}")
        if os.path.exists(model_path) and os.listdir(model_path):
            print(f"LOG: MeloTTS {lang_code} d√©j√† pr√©sent")
            return model_path

        lang_name = "japonais" if lang_code == "JA" else "chinois"
        st.info(f"T√©l√©chargement / pr√©paration du mod√®le MeloTTS {lang_name}...")
        print(f"LOG: Pr√©paration MeloTTS {lang_code}")
        os.makedirs(model_path, exist_ok=True)
        # On suppose que melo.api est d√©j√† install√© et pr√™t √† l'emploi
        print(f"LOG: MeloTTS {lang_code} pr√™t dans {model_path}")
        return model_path

    # Pour les autres langues (MMS-TTS)
    if lang_code not in MMS_TTS_MODELS:
        st.error(f"Pas de mod√®le MMS-TTS d√©fini pour {lang_code}")
        return None

    model_name = MMS_TTS_MODELS[lang_code]
    model_path = os.path.join(CACHE_DIR, f"mms_{lang_code}")

    if not os.path.exists(model_path) or not os.listdir(model_path):
        st.info(f"T√©l√©chargement du mod√®le MMS-TTS pour {lang_code}...")
        print(f"LOG: T√©l√©chargement MMS-TTS {lang_code} depuis {model_name}")
        # Pr√©charger le tokenizer et le mod√®le VITS
        AutoTokenizer.from_pretrained(model_name, cache_dir=model_path)
        VitsModel.from_pretrained(model_name, cache_dir=model_path)
        print(f"LOG: MMS-TTS {lang_code} t√©l√©charg√© dans {model_path}")
    else:
        print(f"LOG: MMS-TTS {lang_code} d√©j√† pr√©sent dans {model_path}")

    return model_path

def speak_mms(text, lang_code):
    """
    Synth√®se vocale MMS-TTS utilisant VitsModel et AutoTokenizer.
    """
    model_id = MMS_TTS_MODELS[lang_code]
    device = "cuda" if torch.cuda.is_available() else "cpu"

    try:
        st.info(f"G√©n√©ration audio pour {lang_code} avec {model_id}...")
        # Chargement du mod√®le Vits
        model = VitsModel.from_pretrained(model_id).to(device)
        tokenizer = AutoTokenizer.from_pretrained(model_id)

        # Tokenisation du texte
        inputs = tokenizer(text, return_tensors="pt").to(device)

        # G√©n√©ration de la voix
        with torch.no_grad():
            waveform = model(**inputs).waveform

        # Cr√©ation du fichier temporaire et lecture audio
        tmp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
        sampling_rate = model.config.sampling_rate
        sf.write(tmp_wav, waveform.squeeze().cpu().numpy(), sampling_rate)
        st.audio(tmp_wav)
        print(f"LOG: Lecture audio termin√©e, fichier temporaire: {tmp_wav}")

        os.unlink(tmp_wav)

    except Exception as e:
        st.error(f"Erreur lors de la synth√®se vocale : {e}")
        print(f"ERROR: {e}")




def speak_japanese_melo(text):
    """
    Synth√®se vocale Japonais via script externe MeloTTS
    """
    script_path = os.path.join(base_path, "generate_tts_jp.py")
    tmp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
    try:
        cmd = ["python", script_path, json.dumps({'text': text, 'output_path': tmp_wav})]
        result = subprocess.run(cmd, capture_output=True, text=True, check=False)
        if result.returncode == 0 and "Success" in result.stdout:
            st.audio(tmp_wav)
            st.success("‚úÖ Synth√®se vocale japonaise r√©ussie")
        else:
            st.error(f"Erreur script Japonais : {result.stderr}")
        os.unlink(tmp_wav)
    except Exception as e:
        st.error(f"Erreur TTS Japonais : {e}")




def speak_chinese_melo(text):
    script_path = os.path.join(base_path, "generate_tts_ch.py")
    tmp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
    try:
        # Forcer CPU et d√©sactiver MPS sur Mac
        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = ""        # D√©sactive CUDA
        env["PYTORCH_ENABLE_MPS_FALLBACK"] = "0" # D√©sactive MPS fallback

        cmd = ["python", script_path, json.dumps({'text': text, 'output_path': tmp_wav})]
        result = subprocess.run(cmd, capture_output=True, text=True, check=False, env=env)
        if result.returncode == 0 and "Success" in result.stdout:
            st.audio(tmp_wav)
            st.success("‚úÖ Synth√®se vocale chinoise r√©ussie")
        else:
            st.error(f"Erreur TTS chinois : {result.stderr}")
        os.unlink(tmp_wav)
    except Exception as e:
        st.error(f"Erreur TTS Chinois : {e}")





# =======================
# FONCTIONS DE LOGIQUE
# =======================

def detect_silence(audio_chunk, threshold=SILENCE_THRESHOLD):
    rms = np.sqrt(np.mean(audio_chunk ** 2))
    return rms < threshold

def record_with_silence_detection(duration=MAX_RECORDING_TIME, sample_rate=SAMPLE_RATE):
    print("LOG: D√©marrage de l'enregistrement audio...")
    audio_queue = queue.Queue()
    recording = []
    def callback(indata, frames, time, status):
        if status:
            print(f"LOG: Audio status: {status}")
        audio_queue.put(indata.copy())
    with sd.InputStream(callback=callback, channels=1, samplerate=sample_rate):
        start = time.time()
        silence_start = None
        while True:
            try:
                chunk = audio_queue.get(timeout=0.1)
                recording.append(chunk)
                elapsed = time.time() - start
                if elapsed >= duration:
                    print("LOG: Dur√©e maximale atteinte")
                    break
                if detect_silence(chunk):
                    if silence_start is None:
                        silence_start = time.time()
                    elif time.time() - silence_start >= SILENCE_DURATION:
                        print("LOG: Silence d√©tect√©, arr√™t de l'enregistrement")
                        break
                else:
                    silence_start = None
            except queue.Empty:
                continue
    if recording:
        audio_data = np.concatenate(recording, axis=0)
        print(f"LOG: Enregistrement termin√©, {len(audio_data)} √©chantillons captur√©s")
        return (audio_data * 32767).astype(np.int16)
    print("LOG: Aucun audio enregistr√©")
    return None

def transcribe(lang_name):
    print(f"LOG: D√©but de la transcription pour {lang_name}")
    wav_path = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name

    # Enregistrement audio avec d√©tection du silence
    audio = record_with_silence_detection()
    if audio is None:
        st.warning("Aucun audio d√©tect√©")
        return ""

    # Sauvegarde en WAV temporaire
    with wave.open(wav_path, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(SAMPLE_RATE)
        wf.writeframes(audio.tobytes())

    # Cas particulier : Japonais ou Mandarin -> toujours Whisper
    if lang_name in ["Japonais", "Mandarin"]:
        print(f"LOG: Transcription {lang_name} avec Whisper (pas de mod√®le Vosk ou par pr√©f√©rence)")
        model = load_whisper_model()
        lang_code = "ja" if lang_name == "Japonais" else "zh"
        result = model.transcribe(wav_path, language=lang_code)
        os.unlink(wav_path)
        transcription = result["text"]
        print(f"LOG: Transcription Whisper ({lang_code}) termin√©e: {transcription}")
        return transcription

    # Pour les autres langues -> essai Vosk
    vosk_model = download_vosk_model(lang_name)
    if vosk_model:
        print(f"LOG: Transcription {lang_name} avec Vosk")
        rec = KaldiRecognizer(vosk_model, SAMPLE_RATE)
        rec.SetWords(True)
        results = []
        with wave.open(wav_path, "rb") as wf:
            while True:
                data = wf.readframes(4000)
                if len(data) == 0:
                    break
                if rec.AcceptWaveform(data):
                    res = json.loads(rec.Result())
                    if 'text' in res:
                        results.append(res['text'])
            final = json.loads(rec.FinalResult())
            if 'text' in final:
                results.append(final['text'])
        os.unlink(wav_path)
        transcription = " ".join(results)
        print(f"LOG: Transcription Vosk termin√©e: {transcription}")
        return transcription

    # Fallback Whisper si Vosk indisponible
    print(f"LOG: Transcription {lang_name} avec Whisper (fallback)")
    model = load_whisper_model()
    code = LANGUAGES_DATA[lang_name]["seamless_code"].strip("_")
    result = model.transcribe(wav_path, language=code)
    os.unlink(wav_path)
    transcription = result["text"]
    print(f"LOG: Transcription Whisper (fallback) termin√©e: {transcription}")
    return transcription

def translate(text, src, tgt, model, processor):
    print(f"LOG: Traduction de {src} vers {tgt}")
    inputs = processor(text=text.strip(), src_lang=LANGUAGES_DATA[src]["seamless_code"], return_tensors="pt")
    with torch.no_grad():
        output = model.generate(**inputs,
                                tgt_lang=LANGUAGES_DATA[tgt]["seamless_code"],
                                generate_speech=False,
                                max_length=256)
    translated = processor.decode(output.sequences[0].tolist(), skip_special_tokens=True)
    clean = re.sub(r'__[\w_]+__', '', translated).strip()
    print(f"LOG: Traduction termin√©e: {clean}")
    return clean

# =======================
# INTERFACE STREAMLIT AM√âLIOR√âE
# =======================

st.title("üß† Interpr√®te Multilingue (100% Offline)")
st.markdown("**Version am√©lior√©e avec d√©tection de silence et TTS optimis√©**")
st.markdown("---")

# Configuration des langues
col1, col2 = st.columns(2)
with col1:
    input_lang = st.selectbox("üé§ Langue de l'interlocuteur :", list(LANGUAGES_DATA.keys()), index=0)
with col2:
    target_lang = st.selectbox("üî§ Langue de sortie :", list(LANGUAGES_DATA.keys()), index=1)

# V√©rification que les langues sont diff√©rentes
if input_lang == target_lang:
    st.warning("‚ö†Ô∏è Les langues source et cible sont identiques")

# Bouton pour valider et charger les mod√®les
if st.button("‚úÖ Valider et charger mod√®les"):
    st.session_state.selected_pair = (input_lang, target_lang)
    st.info(f"T√©l√©chargement des mod√®les pour {input_lang} et {target_lang}...")
    download_vosk_model(input_lang)
    download_mms_model(LANGUAGES_DATA[input_lang]["tts_lang_code"])
    download_mms_model(LANGUAGES_DATA[target_lang]["tts_lang_code"])
    st.success("‚úÖ Mod√®les t√©l√©charg√©s et pr√™ts !")

st.markdown("---")

# === √âTAPE 1: TRANSCRIPTION ===
if "selected_pair" in st.session_state:
    st.subheader("1. üé§ Transcription avec d√©tection de silence")
    with st.expander("‚ÑπÔ∏è Param√®tres d'√©coute"):
        st.write(f"‚Ä¢ Dur√©e maximale : {MAX_RECORDING_TIME} s")
        st.write(f"‚Ä¢ Arr√™t automatique : {SILENCE_DURATION} s de silence")
        st.write(f"‚Ä¢ Seuil de silence : {SILENCE_THRESHOLD}")

    if st.button("üé§ D√©marrer √©coute intelligente"):
        with st.spinner("Enregistrement en cours..."):
            transcription = transcribe(st.session_state.selected_pair[0])
            if transcription:
                st.session_state.last_transcription = transcription
                st.success(f"‚úÖ Transcription r√©ussie ({st.session_state.selected_pair[0]})")
                st.write(f"**Texte transcrit :** {transcription}")
            else:
                st.error("‚ùå Aucun texte d√©tect√©")

# === √âTAPE 2: TRADUCTION ===
if st.session_state.get("last_transcription"):
    st.markdown("---")
    st.subheader("2. üåç Traduction")
    st.write(f"Traduction : **{st.session_state.selected_pair[0]} ‚Üí {st.session_state.selected_pair[1]}**")
    st.write(f"**Texte √† traduire :** {st.session_state.last_transcription}")

    if st.button("üåç Traduire"):
        with st.spinner("Traduction en cours..."):
            model, processor = get_translator_pipeline()
            translated_text = translate(
                st.session_state.last_transcription,
                st.session_state.selected_pair[0],
                st.session_state.selected_pair[1],
                model, processor
            )
            if translated_text:
                st.session_state.translated = translated_text
                st.success("‚úÖ Traduction r√©ussie")
                st.write(f"**Traduction :** {translated_text}")
            else:
                st.error("‚ùå √âchec de la traduction")

# === √âTAPE 3: SYNTH√àSE VOCALE ===
if st.session_state.get("translated"):
    st.markdown("---")
    st.subheader("3. üîä Lecture audio de la traduction")
    st.write(f"**Texte √† prononcer :** {st.session_state.translated}")
    target_tts_code = LANGUAGES_DATA[st.session_state.selected_pair[1]]["tts_lang_code"]
    if st.button("üîä Lire traduction"):
        if target_tts_code == "JA":
            speak_japanese_melo(st.session_state.translated)
        elif target_tts_code == "ZH":
            speak_chinese_melo(st.session_state.translated)
        else:
            speak_mms(st.session_state.translated, target_tts_code)




# === √âTAPE 4: VOTRE R√âPONSE ===
st.markdown("---")
st.subheader("4. üìù Votre r√©ponse")
st.info(f"√âcrivez votre r√©ponse en **{st.session_state.get('selected_pair', ('', ''))[1]}**")

user_response = st.text_area("Tapez votre r√©ponse ici...", key="user_response_text")
if st.button("üîÅ Traduire et prononcer votre r√©ponse"):
    if user_response.strip():
        model, processor = get_translator_pipeline()
        translated_response = translate(
            user_response,
            st.session_state.selected_pair[1],
            st.session_state.selected_pair[0],
            model, processor
        )
        st.session_state.translated_response = translated_response
        st.write(f"**R√©ponse traduite :** {translated_response}")

        # D√©tection de la langue pour TTS
        target_lang_code = LANGUAGES_DATA[st.session_state.selected_pair[0]]["tts_lang_code"]

        if target_lang_code == "JA":
            # Utilisation de MeloTTS japonais via script externe
            tmp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
            data = {"text": translated_response, "output_path": tmp_wav}
            try:
                result = subprocess.run(
                    ["python", "generate_tts_jp.py", json.dumps(data)],
                    capture_output=True, text=True
                )
                if result.returncode == 0 and "Success" in result.stdout:
                    st.audio(tmp_wav)
                    st.success("‚úÖ Synth√®se vocale japonaise r√©ussie")
                else:
                    st.error(f"Erreur TTS japonais : {result.stderr}")
                os.unlink(tmp_wav)
            except Exception as e:
                st.error(f"Erreur lors de l'appel √† MeloTTS japonais : {e}")

        elif target_lang_code == "ZH":
            # Utilisation de MeloTTS chinois via script externe (CPU forc√©, MPS d√©sactiv√©)
            tmp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=".wav").name
            data = {"text": translated_response, "output_path": tmp_wav}
            try:
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = ""        # D√©sactive CUDA
                env["PYTORCH_ENABLE_MPS_FALLBACK"] = "0" # D√©sactive MPS fallback
                result = subprocess.run(
                    ["python", "generate_tts_ch.py", json.dumps(data)],
                    capture_output=True, text=True,
                    env=env
                )
                if result.returncode == 0 and "Success" in result.stdout:
                    st.audio(tmp_wav)
                    st.success("‚úÖ Synth√®se vocale chinoise r√©ussie")
                else:
                    st.error(f"Erreur TTS chinois : {result.stderr}")
                os.unlink(tmp_wav)
            except Exception as e:
                st.error(f"Erreur lors de l'appel √† MeloTTS chinois : {e}")

        else:
            # Synth√®se vocale MMS pour les autres langues
            speak_mms(translated_response, target_lang_code)
    else:
        st.warning("Veuillez taper votre r√©ponse avant de continuer.")






# === DEBUG / INFO SESSION ===
with st.expander("üîß Informations de d√©bogage"):
    st.write(st.session_state)

# === R√âINITIALISATION ===
st.markdown("---")
if st.button("üîÑ R√©initialiser la session"):
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    st.success("Session r√©initialis√©e")
    st.experimental_rerun()